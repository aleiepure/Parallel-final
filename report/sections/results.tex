\section{Results and Conclusions}

\begin{table}[!t]
    \renewcommand{\arraystretch}{1.3}
    \caption{\label{table:results}Results of the experiments - Execution times.}
    \centering
    \begin{tabular}{c|c|c|c|c}
        \hline
        \bfseries  & \bfseries Cores & \bfseries Time [\SI{}{\milli\second}] & \bfseries Speedup [$\times$] & \bfseries Efficiency [$\%$]\\
        \hline
        Sequential & - & 758 & 1.00 & 100\\
        \hline
                & 2  & 397 & 1.91  & 95.55\\
                & 4  & 200 & 3.79  & 94.75\\
        OpenMP & 8  & 104 & 7.27  & 90.81\\
                & 16 & 56  & 13.62 & 85.10\\
                & 32 & 33  & 22.74 & 71.06\\
                & 64 & 31  & 24.72 & 38.62\\
        \hline
                & 2  & 415 & 1.83  & 91.33\\
                & 4  & 257 & 2.95  & 73.64\\
        MPI    & 8  & 151 & 5.02  & 62.75\\
                & 16 & 116 & 6.52  & 40.72\\
                & 32 & 100 & 7.58  & 23.69\\
                & 64 & 92  & 8.27  & 12.92\\
        \hline
        CUDA   & - & 10 & 75.80 & - \\
        \hline
    \end{tabular}
\end{table}

OpenMP and MPI versions were tested multiple times each with a different amounts
of threads and processes, respectively. On the other hand, the sequential and CUDA %
versions were tested only once. The results of three runs were averaged to obtain %
a more reliable estimate of the performance. The speedup was calculated as the %
ratio between the execution time of the sequential version and the execution time %
of the parallel version. The efficiency was calculated as the ratio between the %
speedup and the number of threads or processes. The results are shown in Table \ref{table:results}. %

\input{assets/time}
\input{assets/speedup}
\input{assets/efficiency}

The experimental results demonstrate the performance characteristics of different %
parallelization techniques for the image processing algorithm. The sequential %
implementation serves as the baseline. Assuming ideal conditions, parallel execution %
times should decrease linearly as the number of cores increase. However, our experiment %
demonstrates that this is not always the case. 

Comparing the execution times, we can see that both OpenMP %
and MPI decrease as the number of cores increases, as shown in Fig. \ref{fig:plotTime}. %
Around 16 cores, the execution times tend to stabile due to Amdahl's Law. Increasing %
the number of cores can reduce the execution time only as long as the %
sequential fraction remains a significant portion of the total time. %
Once it becomes negligible compared to the optimized %
parallel time, further increases in cores will not lead to significant %
additional improvements in the overall time.

Analyzing the results on OpenMP, from 32 to 64 %
cores, the efficiency drops significantly as is observable in Fig. \ref{fig:plotEfficiency}. %
Similarly, the MPI implementation shows a more pronounced drop between 8 and 64 %
processes. This is likely due to increased overhead and resource contention, which %
diminishes the returns in speedup.
Discussing the speedup, OpenMP adheres to the ideal trend up to 32 cores before %
stabilizing. On the other hand, MPI exhibits limited speedup starting from 8 %
cores, remaining constant thereafter. This trend is also evident in Fig. \ref{fig:plotSpeedup}. %
The CUDA implementation, achieves the highest speedup. However the %
resulting efficiency is not directly comparable to the one from the other %
CPU-based implementations in this experiment.

Given the techniques explored, architectures leveraging GPU acceleration, as %
demonstrated in the CUDA implementation, offer the highest speedup, making them %
particularly effective for image processing tasks.

Overall, our findings provide insights into the effectiveness and limitations of %
different parallelization techniques for image processing algorithms. Future %
research could explore optimization strategies to address scalability issues and %
further leverage GPU acceleration for improved efficiency. The experimental results %
underscore the importance of choosing the appropriate parallelization technique %
based on the characteristics of the algorithm and the underlying hardware %
architecture, enabling researchers and developers to make informed decisions to %
optimize performance and address computational challenges in image processing %
tasks.
